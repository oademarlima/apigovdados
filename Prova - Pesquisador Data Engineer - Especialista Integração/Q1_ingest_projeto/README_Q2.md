# Q2 â€“ TransformaÃ§Ãµes (Spark) â†’ Silver & Gold

Este documento explica como executar a Q2, validar os resultados e exportar os *gold tables* para CSV.

---

## 1) PrÃ©-requisitos

- Python (no venv do projeto)
- Java 17 instalado e `JAVA_HOME` configurado  
- Pacotes no venv: `pyspark`, `pyarrow`
- Q1 executada previamente (existÃªncia do *bronze* com CSV/ZIP)

> ðŸ’¡ Ajuste de memÃ³ria recomendado (evita erros `OutOfMemoryError`):
```bash
export SPARK_DRIVER_MEMORY=8g
export _JAVA_OPTIONS="-Xms1g -Xmx8g"
```

---

## 2) Estrutura esperada de pastas

```
Q1_ingest_projeto/
  â”œâ”€â”€ src/
  â”‚   â””â”€â”€ transform_q2.py
  â”œâ”€â”€ resources/
  â”‚   â”œâ”€â”€ dim_uf_regiao.csv
  â”‚   â””â”€â”€ tech_map.json
  â””â”€â”€ data_lake/
      â””â”€â”€ bronze/
          â””â”€â”€ dt_coleta=YYYY-MM-DD/hora=HH-MM-SS/
               â”œâ”€â”€ raw.zip   (opcional)
               â””â”€â”€ extracted/ (opcional, CSVs extraÃ­dos)
```

SaÃ­das criadas pela Q2:
```
data_lake/
  â”œâ”€â”€ silver/acessos_blf/
  â””â”€â”€ gold/
       â”œâ”€â”€ q2_total_acessos_por_regiao/
       â”œâ”€â”€ q2_evolucao_por_tecnologia/
       â””â”€â”€ q2_evolucao_por_regiao_tecnologia/   (se usar --gold-extra)
```

---

## 3) Executar a Q2

Dentro da raiz do projeto (`Q1_ingest_projeto`), com o venv ativado:

### Modo produÃ§Ã£o (particionado por ano)
```bash
python -m src.transform_q2 --datalake ./data_lake
```

### Modo local/dev (gera 1 arquivo por saÃ­da)
```bash
python -m src.transform_q2 --datalake ./data_lake --single-file
```

### Incluir gold extra (regiÃ£o + tecnologia)
```bash
python -m src.transform_q2 --datalake ./data_lake --gold-extra
```

---

## 4) ValidaÃ§Ã£o (visualizaÃ§Ã£o rÃ¡pida no terminal)

Rode o bloco abaixo **direto no terminal**. Ele imprime *schema* e amostras do Silver e dos Golds:

```bash
python - <<'PY'
from pyspark.sql import SparkSession
spark = SparkSession.builder.getOrCreate()

silver = spark.read.parquet("data_lake/silver/acessos_blf")
print("Silver schema:")
silver.printSchema()
print("Amostra (silver):")
silver.select("ano","mes","uf","regiao","tecnologia","acessos").show(10, truncate=False)

g1 = spark.read.parquet("data_lake/gold/q2_total_acessos_por_regiao/*/*")
print("Gold1 - total por regiÃ£o:")
g1.show()

g2 = spark.read.parquet("data_lake/gold/q2_evolucao_por_tecnologia/*/*")
print("Gold2 - evoluÃ§Ã£o por tecnologia:")
g2.orderBy("ano","tecnologia").show(20, truncate=False)

# se rodou com --gold-extra
try:
    g3 = spark.read.parquet("data_lake/gold/q2_evolucao_por_regiao_tecnologia/*/*")
    print("Gold3 - evoluÃ§Ã£o por regiÃ£o + tecnologia:")
    g3.orderBy("ano","regiao","tecnologia").show(20, truncate=False)
except Exception:
    print("Gold extra nÃ£o encontrado (use --gold-extra para gerar).")
PY
```

---

## 5) ExportaÃ§Ã£o para CSV (abrir no Excel etc.)

Gera versÃµes CSV dos Golds em `data_lake/gold_csv/`:

```bash
python - <<'PY'
from pyspark.sql import SparkSession
spark = SparkSession.builder.getOrCreate()

spark.read.parquet("data_lake/gold/q2_total_acessos_por_regiao/*/*") \
     .coalesce(1).write.mode("overwrite").option("header", True) \
     .csv("data_lake/gold_csv/q2_total_por_regiao")

spark.read.parquet("data_lake/gold/q2_evolucao_por_tecnologia/*/*") \
     .coalesce(1).write.mode("overwrite").option("header", True) \
     .csv("data_lake/gold_csv/q2_evolucao_por_tecnologia")

# opcional: gold extra
try:
    spark.read.parquet("data_lake/gold/q2_evolucao_por_regiao_tecnologia/*/*") \
         .coalesce(1).write.mode("overwrite").option("header", True) \
         .csv("data_lake/gold_csv/q2_evolucao_por_regiao_tecnologia")
except Exception:
    pass

print("CSV gerados em data_lake/gold_csv/")
PY
```

SaÃ­da esperada:
```
data_lake/gold_csv/
  â”œâ”€â”€ q2_total_por_regiao/
  â”‚     â””â”€â”€ part-00000-....csv
  â”œâ”€â”€ q2_evolucao_por_tecnologia/
  â”‚     â””â”€â”€ part-00000-....csv
  â””â”€â”€ q2_evolucao_por_regiao_tecnologia/   (se --gold-extra)
        â””â”€â”€ part-00000-....csv
```

---

## 6) Limpar e reprocessar

Para apagar saÃ­das e rodar novamente:

```bash
rm -rf ./data_lake/silver ./data_lake/gold ./data_lake/gold_csv
python -m src.transform_q2 --datalake ./data_lake
```

---

## 7) SoluÃ§Ã£o de problemas (FAQ)

- **Erro: â€œBronze nÃ£o encontrado / Nenhum dt_coleta / Nenhuma pasta hora=â€**  
  â†’ Execute a Q1 primeiro e garanta que existe `data_lake/bronze/dt_coleta=.../hora=...` com CSV/ZIP.

- **Erro: PATH_NOT_FOUND para `extracted/raw.csv`**  
  â†’ A Q2 busca CSVs em `extracted/` **ou** dentro de `raw.zip` **ou** um `raw.csv` solto. Verifique onde estÃ¡ o arquivo real.

- **Mensagem: â€œNenhum arquivo pÃ´de ser padronizado em (ano, mes, acessos)â€**  
  â†’ Os CSVs estÃ£o em formato â€œwideâ€ (colunas `YYYY-MM`). A Q2 jÃ¡ tenta *melt*, mas se o arquivo for muito fora do padrÃ£o, ajuste os cabeÃ§alhos ou confirme separador/encoding.

- **Erro: AMBIGUOUS_REFERENCE `uf` ao *join***  
  â†’ JÃ¡ tratamos no cÃ³digo usando `join(dim, F.upper(F.col("uf")) == dim.uf, "left")` e preservando apenas uma `uf`. Se alterou o script, garanta que o *drop* correto estÃ¡ aplicado.

- **Java nÃ£o instalado / `JAVA_GATEWAY_EXITED`**  
  â†’ Instale Java 17 e configure `JAVA_HOME`. No macOS via Homebrew:  
  ```bash
  brew install openjdk@17
  echo 'export JAVA_HOME=$( /usr/libexec/java_home -v 17 )' >> ~/.zshrc
  echo 'export PATH="$JAVA_HOME/bin:$PATH"' >> ~/.zshrc
  source ~/.zshrc
  ```

- **Aviso: WARN parquet.enable.dictionary**  
  â†’ JÃ¡ corrigido no script com `.config("spark.hadoop.parquet.enable.dictionary","true")`.  
