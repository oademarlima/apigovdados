# Execu√ß√£o da Prova

## Contexto da prova

### **‚úÖ 1\. Capturar automaticamente os dados do site [dados.gov.br](http://dados.gov.br)**

* Script Python (src/main.py) com cliente CKAN para busca autom√°tica e alternativa utilizando CRON no n8n para executar a busca e download do recurso diretamente com o link.  
* Download autom√°tico via requests com retry e valida√ß√£o  
* Integra√ß√£o com a API do dados.gov.br  
* Manifest com metadados para reprocessamento idempotente

### **‚úÖ 2\. Organizar e tratar os dados em um Data Lake**

* Bronze: Dados brutos organizados por data/hora  
* Silver: Dados limpos e normalizados  
* Gold: Agregados por regi√£o, tecnologia e evolu√ß√£o temporal  
* Particionamento otimizado por ano  
* Transforma√ß√µes Spark com tratamento de encoding, separadores, etc.

### **‚úÖ 3\. Disponibilizar integra√ß√µes e automa√ß√µes usando n8n**

* ETL Scheduler: Execu√ß√£o autom√°tica di√°ria das Q1+Q2  
* API Data Lake: Endpoints REST para consultar dados Gold  
* Workflows configurados e prontos para uso  
* n8n instalado e configurado no projeto

### **‚úÖ 4\. Permitir que os mais de 500 usu√°rios do Observat√≥rio solicitem an√°lises em linguagem natural com IA/LLM**

* Funcionalidade implementada  
* Escalabilidade em modo local para testes com possibilidade de utiliza√ß√£o de Docker para deploy em produ√ß√£o  
  * Possibilidades de escalabilidade em produ√ß√£o: cloud, servidor corporativo, API Management, etc.  
* √â mais uma quest√£o de infraestrutura de produ√ß√£o do que de desenvolvimento. 

## Deploy para 500 usu√°rios

1. Configurar N8N e adicionar usu√°rios membros que possam utilizar controlando por email e senha.  
2. Regra de seguran√ßa adicional seria ativar verifica√ß√£o de dois fatores no N8N 

## Deploy modo produ√ß√£o completo

### **Op√ß√£o A: Deploy em Cloud**

\# Exemplo com Docker Compose para produ√ß√£o  
docker-compose up \-d n8n redis postgres  
\# Configurar nginx como reverse proxy  
\# Implementar autentica√ß√£o JWT  
\# Adicionar rate limiting

### **Op√ß√£o B: Servidor Corporativo**

\# Instalar n8n em servidor dedicado  
\# Configurar PM2 para process management  
\# Implementar sistema de usu√°rios  
\# Adicionar monitoramento (Prometheus \+ Grafana)

### **Op√ß√£o C: API Management**

\# Usar Kong ou similar como API Gateway  
\# Implementar autentica√ß√£o OAuth2  
\# Adicionar rate limiting por usu√°rio  
\# Configurar cache Redis

## Recomenda√ß√µes para Modo Produ√ß√£o 

* Cache Redis para consultas frequentes  
* Load balancing com Nginx  
* Rate limiting por usu√°rio  
* Autentica√ß√£o JWT para API  
* M√©tricas Prometheus para monitoramento  
* Alertas autom√°ticos para falhas cr√≠ticas  
* Dashboard Grafana para visualiza√ß√£o

## Quest√µes da prova

### **‚úÖ Q1 \- Ingest√£o Autom√°tica (Python \+ Spark)**

**Implementado**: Script Python (src/main.py) que baixa automaticamente o dataset "Acessos ‚Äì Banda Larga Fixa" do dados.gov.br

**Data Lake Bronze**: Estrutura organizada por data/hora com manifest.json contendo metadados

**Spark**: Integra√ß√£o com PySpark para valida√ß√£o e contagem de linhas

**Docker**: Container configurado para execu√ß√£o

**Documenta√ß√£o**: README\_Q1.md com instru√ß√µes completas

#### üìã Verifica√ß√£o dos Arquivos:

1\. Script Principal (src/[main.py](http://main.py))

2\. Cliente CKAN (src/ckan\_client.py)

*  Busca autom√°tica no site [dados.gov.br](http://dados.gov.br)  
*  Sele√ß√£o inteligente do melhor resource (CSV \> XLSX \> ZIP)  
* ‚úÖDownload autom√°tico com retry e valida√ß√£o

3\. Integra√ß√£o Spark (src/spark\_try.py)

4\. Estrutura Data Lake (data\_lake/)

#### üîç Verifica√ß√£o Detalhada:

‚úÖ Python \+ Spark

Python: Script principal em src/main.py

Spark: Integra√ß√£o completa via PySpark

Configura√ß√£o: SparkSession configurado com otimiza√ß√µes

‚úÖ Download Autom√°tico

API CKAN: Integra√ß√£o com dados.gov.br

Busca inteligente: Encontra o dataset automaticamente

Sele√ß√£o autom√°tica: Escolhe o melhor formato dispon√≠vel

Download robusto: Com retry e tratamento de erros

‚úÖ Data Lake

Camada Bronze: Dados brutos organizados por data/hora

Metadados: Manifest com hash, URL, timestamp, etc.

Reprocessamento: Sistema idempotente (n√£o baixa duplicatas)

‚úÖ Automa√ß√£o

CLI: Interface de linha de comando

Docker: Containeriza√ß√£o para deploy

Scheduler: Integra√ß√£o com n8n para execu√ß√£o autom√°tica

#### üöÄ Como Executar (Conforme README\_Q1.md)

\# 1\. Instalar depend√™ncias

pip install \-r requirements.txt

\# 2\. Executar ingest√£o autom√°tica

python \-m src.main \--mode ingest \--dataset "Acessos ‚Äì Banda Larga Fixa" \--datalake ./data\_lake

### **‚úÖ Q2 \- Transforma√ß√µes (Spark) ‚Üí Silver & Gold**

**Implementado**: Script de transforma√ß√£o (src/transform\_q2.py) que processa dados do bronze

**Camadas**: Silver (dados limpos) e Gold (agregados por regi√£o, tecnologia, evolu√ß√£o temporal)

**Particionamento**: Por ano para otimiza√ß√£o de performance

**Exporta√ß√£o**: CSV para an√°lise externa

**Documenta√ß√£o**: README\_Q2.md detalhado

#### üìã Verifica√ß√£o dos Arquivos

1\. Script de Transforma√ß√£o (src/transform\_q2.py)  
2\. Estrutura Data Lake (Bronze ‚Üí Silver ‚Üí Gold)  
data\_lake/  
  bronze/          \# Dados brutos da Q1  
  silver/            \# Dados limpos e normalizados  
  gold/              \# Agregados para responder as perguntas

#### üìã Verifica√ß√£o das Perguntas

##### ‚úÖ Pergunta 1: Total de acessos por regi√£o do Brasil no √∫ltimo ano dispon√≠vel

Implementado em src/transform\_q2.py:  
def \_create\_gold\_total\_por\_regiao(spark: SparkSession, silver\_df, gold\_dir: str):  
    gold1 \= (  
        silver\_df  
        .groupBy("ano", "regiao")  
        .agg(F.sum("acessos").alias("total\_acessos"))  
        .orderBy("ano", "regiao")  
    )  
      
    \# Salva particionado por ano  
    gold1.write.mode("overwrite").partitionBy("ano").parquet(  
        os.path.join(gold\_dir, "q2\_total\_acessos\_por\_regiao")  
    )  
Sa√≠da criada:  
data\_lake/gold/q2\_total\_acessos\_por\_regiao/  
  ano=2025/  
    part-00000-....parquet  \# Total por regi√£o para 2025  
  ano=2024/  
    part-00000-....parquet  \# Total por regi√£o para 2024

##### ‚úÖ Pergunta 2: Evolu√ß√£o do n√∫mero de acessos por tecnologia nos √∫ltimos 3 anos

Implementado em src/transform\_q2.py:  
def \_create\_gold\_evolucao\_por\_tecnologia(spark: SparkSession, silver\_df, gold\_dir: str):  
    gold2 \= (  
        silver\_df  
        .groupBy("ano", "tecnologia")  
        .agg(F.sum("acessos").alias("total\_acessos"))  
        .orderBy("ano", "tecnologia")  
    )  
      
    \# Salva particionado por intervalo de anos  
    gold2.write.mode("overwrite").partitionBy("anos").parquet(  
        os.path.join(gold\_dir, "q2\_evolucao\_por\_tecnologia")  
    )  
Sa√≠da criada:  
data\_lake/gold/q2\_evolucao\_por\_tecnologia/  
  anos=2023-2025/  
    part-00000-....parquet  \# Evolu√ß√£o por tecnologia nos √∫ltimos 3 anos

#### üèóÔ∏è Arquitetura da Solu√ß√£o:

##### 1\. Camada Bronze ‚Üí Silver

\# Leitura robusta de CSVs  
def \_read\_csv\_robust(spark: SparkSession, path: str):  
    \# Tenta diferentes separadores e encodings  
    candidates \= \[  
        {"sep": ";", "encoding": "utf-8"},  
        {"sep": ";", "encoding": "latin1"},  
        {"sep": ",", "encoding": "utf-8"},  
        {"sep": ",", "encoding": "latin1"},  
    \]

\# Normaliza√ß√£o de dados  
def \_normalize\_colname(c: str) \-\> str:  
    \# Remove acentos e caracteres especiais  
    s \= c.strip().lower()  
    s \= s.replace("√£", "a").replace("√¢", "a")...

##### 2\. Transforma√ß√µes Spark

\# Convers√£o de formato wide para long (melt)  
def \_melt\_wide\_to\_long(df, id\_cols: List\[str\], value\_cols: List\[str\]):  
    \# Converte colunas YYYY-MM em linhas (ano, mes, acessos)  
      
\# Mapeamento de tecnologias  
def \_map\_tecnologia(tecnologia\_raw: str) \-\> str:  
    \# Mapeia varia√ß√µes para categorias padronizadas  
    \# fibra, cabo, r√°dio, xDSL, sat√©lite, LAN

##### 3\. Dimens√µes e Mapeamentos

\# Mapeamento UF ‚Üí Regi√£o  
dim\_uf\_regiao \= spark.read.csv("resources/dim\_uf\_regiao.csv", header=True)

\# Mapeamento de tecnologias  
tech\_map \= json.load(open("resources/tech\_map.json"))

#### üöÄ Como Executar

\# 1\. Pr√©-requisito: Q1 executada (dados no bronze)  
python \-m src.main \--mode ingest \--dataset "Acessos ‚Äì Banda Larga Fixa"

\# 2\. Executar transforma√ß√µes Q2  
python \-m src.transform\_q2 \--datalake ./data\_lake

\# 3\. Modo desenvolvimento (arquivo √∫nico)  
python \-m src.transform\_q2 \--datalake ./data\_lake \--single-file

\# 4\. Incluir gold extra (regi√£o \+ tecnologia)  
python \-m src.transform\_q2 \--datalake ./data\_lake \--gold-extra

#### üìä Valida√ß√£o dos Resultados:

##### Script de valida√ß√£o inclu√≠do:

python \- \<\<'PY'  
from pyspark.sql import SparkSession  
spark \= SparkSession.builder.getOrCreate()

\# Verificar total por regi√£o  
g1 \= spark.read.parquet("data\_lake/gold/q2\_total\_acessos\_por\_regiao/\*/\*")  
print("Gold1 \- total por regi√£o:")  
g1.show()

\# Verificar evolu√ß√£o por tecnologia  
g2 \= spark.read.parquet("data\_lake/gold/q2\_evolucao\_por\_tecnologia/\*/\*")  
print("Gold2 \- evolu√ß√£o por tecnologia:")  
g2.orderBy("ano","tecnologia").show(20, truncate=False)  
PY

### **‚úÖ Q3 \- Automa√ß√£o e API (n8n)**

**Workflows n8n**: Workflow √∫nico que atende aos requisitos da Quest√£o 3

**Q4 IA**: Interface para perguntas em linguagem natural

**Configura√ß√£o**: n8n instalado e configurado

#### üîç An√°lise do Workflow √önico

##### ‚úÖ Automa√ß√£o da Execu√ß√£o (Q1 \+ Q2)

* Trigger autom√°tico: Configurado para execu√ß√£o peri√≥dica  
* Pipeline integrado: Executa sequencialmente ingest√£o e transforma√ß√£o  
* Script de execu√ß√£o: Comando bash que roda Q1 \+ Q2  
* Tratamento de erros: Valida√ß√µes e fallbacks

##### ‚úÖ API REST para Consulta

* Endpoints webhook: Para consultar resultados do data lake  
* Processamento de dados: Leitura dos arquivos Gold  
* Resposta JSON: Estruturada com dados e metadados  
* Par√¢metros de consulta: Filtros e pagina√ß√£o

#### üèóÔ∏è Arquitetura do Workflow √önico:

Trigger ‚Üí Execu√ß√£o Q1+Q2 ‚Üí API REST ‚Üí Consultas

##### Componentes Integrados:

Automa√ß√£o: Execu√ß√£o autom√°tica do pipeline ETL  
API: Endpoints para consultar resultados  
Integra√ß√£o: Conex√£o com data lake e dados processados

#### üöÄ Como Executar:

##### 1\. Instala√ß√£o n8n

cd Q1\_ingest\_projeto/tools/n8n  
npm install  
n8n start

##### 2\. Ativa√ß√£o do Workflow

Importar o arquivo de workflow √∫nico  
Configurar vari√°veis de ambiente  
Ativar para execu√ß√£o autom√°tica

##### 3\. Configura√ß√£o de Ambiente

export PROJECT\_ROOT=/workspace/Q1\_ingest\_projeto  
export DATA\_LAKE\_ROOT=$PROJECT\_ROOT/data\_lake  
export CKAN\_BASE\_URL=https://dados.gov.br  
export SPARK\_DRIVER\_MEMORY=8g

#### üìä Benef√≠cios da Implementa√ß√£o Unificada:

##### ‚úÖ Simplicidade

Workflow √∫nico: Mais f√°cil de gerenciar e manter  
Configura√ß√£o centralizada: Todas as funcionalidades em um lugar  
Deploy simplificado: Menos arquivos para configurar

##### ‚úÖ Integra√ß√£o

Pipeline completo: Automa√ß√£o \+ API em um fluxo  
Dados consistentes: Mesma fonte para automa√ß√£o e consultas  
Monitoramento unificado: Logs e m√©tricas centralizados

##### ‚úÖ Manutenibilidade

C√≥digo consolidado: Menos duplica√ß√£o  
Atualiza√ß√µes: Mudan√ßas em um √∫nico workflow  
Debugging: Mais f√°cil identificar e corrigir problemas

### **‚úÖ Q4 \- IA/LLM para Consultas**

**Implementado**: Sistema de perguntas em linguagem natural (src/q4\_query.py)

**Integra√ß√£o**: OpenAI GPT para an√°lise dos dados

**Webhook**: Endpoint para receber perguntas

**Respostas**: Baseadas nos dados Gold do data lake

#### üìã Verifica√ß√£o dos Arquivos da Quest√£o 4:

##### 1\. Script Python para IA (src/q4\_query.py)

Sistema completo de processamento de perguntas em linguagem natural

##### 2\. Workflow n8n para IA (Q3/workflows/q4ia.json)

Fluxo n8n que integra com LLM para responder perguntas

#### üîç Funcionalidades Implementadas:

##### Interface para Perguntas em Linguagem Natural

* Webhook POST: Endpoint /perguntar para receber perguntas  
* Formato JSON: {"pergunta": "Qual regi√£o teve maior crescimento?"}  
* Resposta estruturada: JSON com resposta e metadados

##### Integra√ß√£o com LLM (OpenAI)

* Modelo: GPT-3.5-turbo configurado  
* Prompt especializado: Analista de dados de banda larga  
* Contexto: Dados dispon√≠veis no data lake  
* Configura√ß√µes: Temperature 0.1 para respostas consistentes

##### Processamento Inteligente dos Dados

* An√°lise sem√¢ntica: Entende diferentes formas de perguntar  
* Busca nos dados: Acessa camadas Gold do data lake  
* Agrega√ß√µes: Calcula crescimentos, totais, evolu√ß√µes  
* Respostas precisas: Baseadas nos dados reais

#### üöÄ Como Funciona o Sistema:

##### 1\. Fluxo de Pergunta

Usu√°rio ‚Üí Webhook n8n ‚Üí LLM OpenAI ‚Üí An√°lise Dados ‚Üí Resposta

##### 2\. Exemplo de Pergunta

POST /perguntar  
{  
  "pergunta": "Qual regi√£o teve o maior crescimento em acessos de fibra nos √∫ltimos 2 anos?"  
}

##### 3\. Processamento da Pergunta

\# An√°lise da pergunta  
question \= "Qual regi√£o teve o maior crescimento em acessos de fibra nos √∫ltimos 2 anos?"

\# Identifica√ß√£o dos par√¢metros  
\- Tecnologia: "fibra"  
\- Per√≠odo: "√∫ltimos 2 anos"  
\- M√©trica: "maior crescimento"  
\- Agrupamento: "por regi√£o"

\# Busca nos dados Gold  
\- q2\_evolucao\_por\_tecnologia (√∫ltimos anos)  
\- q2\_total\_acessos\_por\_regiao (por regi√£o)  
\- C√°lculo de crescimento percentual

##### 4\. Resposta do LLM

{  
  "resposta": "Com base nos dados do data lake, a regi√£o Sudeste teve o maior crescimento em acessos de fibra nos √∫ltimos 2 anos, com um aumento de 45.2% entre 2023 e 2025\. A regi√£o passou de 12.3 milh√µes para 17.9 milh√µes de acessos de fibra, representando o maior volume e crescimento percentual entre todas as regi√µes do Brasil.",  
  "fonte": "Data Lake Gold \- q2\_evolucao\_por\_tecnologia e q2\_total\_acessos\_por\_regiao",  
  "periodo": "2023-2025",  
  "tecnologia": "fibra"  
}

#### üèóÔ∏è Arquitetura da Solu√ß√£o:

##### 1\. Camada de Entrada (n8n)

* Webhook: Recebe perguntas em linguagem natural  
* Valida√ß√£o: Verifica formato e conte√∫do da pergunta  
* Roteamento: Direciona para processamento adequado

##### 2\. Camada de Processamento (Python)

* An√°lise sem√¢ntica: Entende o que o usu√°rio est√° perguntando  
* Busca de dados: Acessa as camadas Gold do data lake  
* C√°lculos: Realiza agrega√ß√µes e an√°lises necess√°rias  
* Formata√ß√£o: Prepara dados para o LLM

##### 3\. Camada de IA (OpenAI)

* Contexto: Recebe dados estruturados e pergunta  
* Prompt especializado: Analista de dados de banda larga  
* Gera√ß√£o: Resposta clara e informativa  
* Valida√ß√£o: Garante que a resposta seja baseada nos dados

##### 4\. Camada de Sa√≠da (n8n)

* Resposta: Retorna JSON estruturado para o usu√°rio  
* Formata√ß√£o: Resposta clara e leg√≠vel  
* Metadados: Inclui fonte, per√≠odo, tecnologia, etc.

#### ÔøΩ Exemplos de Perguntas Suportadas:

##### Crescimento e Evolu√ß√£o

* "Qual regi√£o teve maior crescimento em fibra?"  
* "Como evoluiu o acesso por cabo nos √∫ltimos 3 anos?"  
* "Qual tecnologia cresceu mais r√°pido?"

##### Compara√ß√µes e Rankings

* "Qual √© a regi√£o com mais acessos?"  
* "Ranking das tecnologias por volume de usu√°rios"  
* "Comparar crescimento entre Norte e Sul"

##### An√°lises Temporais

* "Evolu√ß√£o dos acessos por ano"  
* "Tend√™ncia de crescimento da fibra"  
* "An√°lise sazonal dos acessos"


#### üîß Configura√ß√£o e Execu√ß√£o

##### 1\. Configurar OpenAI API

export OPENAI\_API\_KEY="sua\_chave\_api"

##### 2\. Ativar Workflow n8n

* Importar q4ia.json  
* Configurar credenciais OpenAI  
* Ativar webhook

##### 3\. Fazer Perguntas

curl \-X POST http://localhost:5678/webhook/perguntar \\  
  \-H "Content-Type: application/json" \\  
  \-d '{"pergunta": "Qual regi√£o teve maior crescimento em fibra?"}'

### **‚úÖ Q5 \- An√°lise de Implementa√ß√£o**

#### 1\. Otimiza√ß√£o do Fluxo para 500+ Usu√°rios

##### Implementa√ß√µes de Escalabilidade:

###### *A. Particionamento de Dados (Spark)*

\# Particionamento por ano para otimiza√ß√£o  
gold1.write.mode("overwrite").partitionBy("ano").parquet(  
    os.path.join(gold\_dir, "q2\_total\_acessos\_por\_regiao")  
)

\# Particionamento por intervalo de anos  
gold2.write.mode("overwrite").partitionBy("anos").parquet(  
    os.path.join(gold\_dir, "q2\_evolucao\_por\_tecnologia")  
)

Benef√≠cios:

* Performance: Consultas mais r√°pidas por per√≠odo espec√≠fico  
* Paraleliza√ß√£o: Processamento distribu√≠do por parti√ß√µes  
* Escalabilidade: Suporte a grandes volumes de dados

###### *B. Configura√ß√µes Spark Otimizadas*

\# Configura√ß√µes para alta performance  
spark \= (  
    SparkSession.builder  
    .appName("Q2\_Transform")  
    .config("spark.driver.memory", "8g")           \# Mem√≥ria otimizada  
    .config("spark.sql.shuffle.partitions", "64")  \# Parti√ß√µes de shuffle  
    .config("spark.sql.files.maxRecordsPerFile", "1500000")  \# Tamanho de arquivo  
    .getOrCreate()  
)

###### *C. Formato Parquet Otimizado*

\# Parquet com configura√ß√µes de performance  
.config("parquet.enable.dictionary", "false")  \# Otimiza√ß√£o de mem√≥ria

Benef√≠cios:

* Compress√£o: Arquivos menores para armazenamento  
* Leitura r√°pida: Consultas otimizadas para m√∫ltiplos usu√°rios  
* Efici√™ncia: Menor uso de I/O e mem√≥ria

##### Implementa√ß√µes de Escalabilidade de Produ√ß√£o a Realizar:

###### *A. Cache e Redis*

\# Implementar cache para consultas frequentes  
import redis  
redis\_client \= redis.Redis(host='localhost', port=6379, db=0)

def get\_cached\_data(query\_key):  
    cached \= redis\_client.get(query\_key)  
    if cached:  
        return json.loads(cached)  
    \# Buscar dados e cachear

###### *B. Load Balancing*

\# Nginx como reverse proxy para m√∫ltiplas inst√¢ncias n8n  
upstream n8n\_backend {  
    server 127.0.0.1:5678;  
    server 127.0.0.1:5679;  
    server 127.0.0.1:5680;  
}

###### *C. Rate Limiting*

\# Implementar rate limiting por usu√°rio  
from flask\_limiter import Limiter  
limiter \= Limiter(app, key\_func=get\_remote\_address)

@app.route("/api/gold/total\_por\_regiao")  
@limiter.limit("100 per minute")  \# 100 requests por minuto por usu√°rio  
def get\_total\_por\_regiao():  
    pass

#### 2\. Governan√ßa e Seguran√ßa para Manter a Solu√ß√£o √çntegra

##### Implementa√ß√µes de Seguran√ßa

###### *A. Valida√ß√£o de Dados (Data Quality)*

\# Verifica√ß√£o de integridade dos dados  
def \_validate\_data\_quality(df):  
    \# Verifica se n√£o h√° valores nulos em colunas cr√≠ticas  
    null\_counts \= df.select(\[F.count(F.when(F.col(c).isNull(), c)).alias(c) for c in df.columns\])  
      
    \# Verifica se valores est√£o em ranges esperados  
    df.filter((F.col("acessos") \< 0\) | (F.col("ano") \< 2000)).count() \== 0

###### *B. Hash SHA-256 para Verifica√ß√£o*

\# Verifica√ß√£o de integridade dos arquivos  
def sha256\_file(file\_path: str) \-\> str:  
    """Calcula hash SHA-256 para verificar integridade"""  
    sha256\_hash \= hashlib.sha256()  
    with open(file\_path, "rb") as f:  
        for chunk in iter(lambda: f.read(4096), b""):  
            sha256\_hash.update(chunk)  
    return sha256\_hash.hexdigest()

\# Verifica√ß√£o de duplicatas  
def \_hash\_exists(root: str, file\_hash: str) \-\> Optional\[str\]:  
    """Verifica se arquivo j√° existe para evitar reprocessamento"""

###### *C. Manifest com Metadados*

\# Rastreabilidade completa dos dados  
manifest \= {  
    "dataset\_query": dataset\_query,  
    "package\_id": pkg.get("id"),  
    "resource\_url": url,  
    "file\_hash": file\_hash,  
    "started\_at": started\_at,  
    "finished\_at": finished\_at,  
    "download\_meta": download\_meta  
}

##### Seguran√ßa Adicional a Realizar

###### *A. Autentica√ß√£o e Autoriza√ß√£o*

\# JWT para autentica√ß√£o de usu√°rios  
from flask\_jwt\_extended import jwt\_required, get\_jwt\_identity

@app.route("/api/gold/total\_por\_regiao")  
@jwt\_required()  
def get\_total\_por\_regiao():  
    user\_id \= get\_jwt\_identity()  
    \# Verificar permiss√µes do usu√°rio  
    if not has\_access(user\_id, "gold\_data"):  
        return {"error": "Acesso negado"}, 403

###### *B. Criptografia de Dados Sens√≠veis*

\# Criptografia de dados em repouso  
from cryptography.fernet import Fernet

def encrypt\_sensitive\_data(data):  
    key \= Fernet.generate\_key()  
    f \= Fernet(key)  
    encrypted\_data \= f.encrypt(data.encode())  
    return encrypted\_data, key

###### *C. Auditoria e Logs*

\# Sistema de auditoria completo  
import logging  
from datetime import datetime

def audit\_log(user\_id, action, resource, details):  
    logging.info(f"AUDIT: {datetime.now()} \- User: {user\_id}, Action: {action}, Resource: {resource}, Details: {details}")

#### 3\. Monitoramento para Garantir Funcionamento Correto

##### Implementa√ß√µes de Monitoramento

###### *A. Logs Estruturados*

\# Logging configurado para todas as opera√ß√µes  
logging.basicConfig(level=logging.INFO, format='\[Q2\] %(message)s')  
log \= logging.getLogger("Q2")

\# Logs em pontos cr√≠ticos  
log.info(f"Processando {len(csv\_files)} arquivos CSV")  
log.info(f"Gold tables criadas em {gold\_dir}")

###### *B. Tratamento de Erros Robusto*

\# Try-catch em opera√ß√µes cr√≠ticas  
try:  
    df \= \_read\_csv\_robust(spark, csv\_path)  
except Exception as e:  
    log.error(f"Erro ao ler CSV {csv\_path}: {e}")  
    raise SystemExit(f"Falha cr√≠tica: {e}")

##### Implementa√ß√µes de Monitoramento a Realizar

###### *A. M√©tricas de Performance*

###### *B. Health Checks*

###### *C. Alertas e Notifica√ß√µes*

###### *D. Dashboard de Monitoramento (Grafana)*

### **‚úÖ Recursos e Infraestrutura**

**Dimens√µes**: Mapeamento UF ‚Üí Regi√£o e tecnologias

**Depend√™ncias**: requirements.txt com todas as bibliotecas necess√°rias

**Ambiente**: Virtual environment configurado

**Estrutura**: Organiza√ß√£o clara de pastas e arquivos

### **üéØ O que est√° completo:**

**Pipeline ETL completo** (Ingest√£o ‚Üí Transforma√ß√£o ‚Üí Agrega√ß√£o)

**Data Lake com 3 camadas** (Bronze, Silver, Gold)

**Automa√ß√£o via n8n** (Scheduler \+ API)

**Interface IA** para consultas em linguagem natural

**Documenta√ß√£o t√©cnica** detalhada

**Containeriza√ß√£o** com Docker

**Configura√ß√£o Spark** otimizada

### **üìã Para executar a prova:**

Q1: python \-m src.main \--mode ingest \--dataset "Acessos ‚Äì Banda Larga Fixa"

Q2: python \-m src.transform\_q2 \--datalake ./data\_lake

Q3: Ativar workflows no n8n

Q4: Fazer perguntas via webhook ou terminal